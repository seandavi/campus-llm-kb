[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Large Language Model Policy and Practice",
    "section": "",
    "text": "1 Overview\nLarge language models, like ChatGPT, have garnered significant interest due to their human-like language generation and immense natural language processing capabilities. These models offer opportunities to revolutionize healthcare by enhancing clinical decision-making, patient care, and medical research. However, implementing them also poses technical, ethical, legal, and social challenges.\nImplementing large language models in healthcare presents various challenges and opportunities. With careful consideration and mitigation of these challenges, these models have the potential to transform healthcare delivery and improve patient outcomes. It is crucial for healthcare organizations to weigh the risks and benefits of implementation and prioritize ethical and responsible use.\nAs healthcare providers increasingly depend on large language models, ensuring transparency, explainability, and unbiased models is critical. Researchers and developers must collaborate with healthcare providers and patients to align language model development and implementation with ethical principles and patient needs.\nIn summary, the use large language models in healthcare is a complex and rapidly evolving landscape. By addressing the technical, ethical, legal, and social implications, healthcare organizations can harness their full potential to enhance patient outcomes, medical research, business operations, and education."
  },
  {
    "objectID": "index.html#technical-challenges-and-opportunities",
    "href": "index.html#technical-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "1.1 Technical Challenges and Opportunities",
    "text": "1.1 Technical Challenges and Opportunities\nDeveloping and implementing large language models entail considerable computational power for training and inference. These models demand extensive data and computational resources, but recent advancements in deep learning frameworks and cloud computing have facilitated their large-scale deployment.\nBias in language models is another technical challenge. Models trained on biased data can produce biased outcomes, potentially leading to incorrect clinical decisions or reinforcing health disparities. Researchers have proposed various techniques to mitigate bias, such as data augmentation, adversarial training, and fairness constraints."
  },
  {
    "objectID": "index.html#ethical-challenges-and-opportunities",
    "href": "index.html#ethical-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "1.2 Ethical Challenges and Opportunities",
    "text": "1.2 Ethical Challenges and Opportunities\nImplementing large language models in healthcare raises ethical concerns like patient privacy, informed consent, and fairness. Models require vast amounts of data, including personal health information, which can compromise patient privacy and data protection. Patients may also be unaware of how their data is used or may not have provided informed consent.\nConversely, large language models present ethical opportunities. They can generate natural language explanations for clinical decisions, improving transparency and trust between patients and providers. Furthermore, these models can identify and address health disparities by analyzing large-scale data and developing targeted interventions."
  },
  {
    "objectID": "index.html#legal-challenges-and-opportunities",
    "href": "index.html#legal-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "1.3 Legal Challenges and Opportunities",
    "text": "1.3 Legal Challenges and Opportunities\nLegal challenges include liability and regulatory compliance. If language models contribute to clinical decisions, providers may be held liable for adverse outcomes. Compliance with existing regulations, such as HIPAA, is also crucial.\nOn the other hand, legal opportunities arise from using large language models to analyze extensive healthcare data, identifying potential fraud or abuse, and enhancing healthcare delivery efficiency and effectiveness."
  },
  {
    "objectID": "index.html#social-challenges-and-opportunities",
    "href": "index.html#social-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "1.4 Social Challenges and Opportunities",
    "text": "1.4 Social Challenges and Opportunities\nSocial challenges involve potential job displacement and exacerbation of healthcare disparities. Language models could automate healthcare jobs and, if biased, reinforce existing disparities, particularly in marginalized communities.\nHowever, social opportunities also emerge, such as improving healthcare accessibility for at-risk or disadvantaged populations and enhancing healthcare service quality through personalized treatment recommendations and identifying areas for improvement in healthcare delivery or clinical operations."
  },
  {
    "objectID": "index.html#use-in-medical-education",
    "href": "index.html#use-in-medical-education",
    "title": "Large Language Model Policy and Practice",
    "section": "1.5 Use in Medical Education",
    "text": "1.5 Use in Medical Education\nLarge language models can also play a significant role in medical education settings. They can assist in developing personalized learning pathways, providing instant feedback on complex clinical scenarios, and facilitating access to a wealth of medical knowledge. By incorporating these models into medical curricula, educators can enhance the learning experience and better prepare future healthcare professionals."
  },
  {
    "objectID": "framework.html#domains",
    "href": "framework.html#domains",
    "title": "2  The Framework",
    "section": "2.1 Domains",
    "text": "2.1 Domains\nThe implementation plan for integrating AI and Large Language Models into an academic medical system consists of four main domains:\n\nEducation\nResearch\nClinical\nBusiness Operations\n\n\n2.1.1 Education\nThis domain includes all activities related to teaching, learning, and evaluation within the institution. It also encompasses the development of new educational programs and the management of existing ones.\n\n\n2.1.2 Research\nThis domain focuses on the practice of the basic, clinical, and translational research programs within the institution. In addition, it includes the management of research grants, the development of new research programs, and the dissemination of research findings.\n\n\n2.1.3 Clinical\nThis domain encompasses all activities related to patient care, including the management and implementation of clinical services, decision support and clinical decision-making, automation, and point-of-care or electronic patient support.\n\n\n2.1.4 Business Operations\nThis domain focuses on the management of the institution’s business operations, including finance, human resources, information technology, and facilities management. It also includes the development of new business processes and the management of existing ones."
  },
  {
    "objectID": "framework.html#workstreams",
    "href": "framework.html#workstreams",
    "title": "2  The Framework",
    "section": "2.2 Workstreams",
    "text": "2.2 Workstreams\nWithin each domain, we have identified five workstreams that are critical for the successful implementation of AI and Large Language Models. These workstreams are:\n\nData Access & Use\nIT, Security, & Infrastructure\nEthical, Legal, & Social\nTraining & Workforce Development\nProject Management & Support Personnel\n\n\n2.2.1 Data Access & Use\nThis workstream focuses on managing and optimizing data access, use, and sharing within the academic medical system. It ensures that data is available, reliable, and secure for AI integration and that the necessary infrastructure is in place to support data-driven activities.\n\n\n2.2.2 IT, Security, & Infrastructure\nThis workstream addresses the technical aspects of AI integration, including the development and maintenance of IT systems, ensuring data security, and providing the necessary hardware and software infrastructure to support AI and Large Language Models.\n\n\n2.2.3 Ethical, Legal, & Social\nThis workstream focuses on the ethical, legal, and social implications of AI integration in the academic medical system with domain-specific focus as appropriate. It aims to ensure that AI is used responsibly and ethically and that any legal and social concerns are addressed proactively.\n\n\n2.2.4 Training & Workforce Development\nThis workstream is dedicated to developing the skills and knowledge of domain community members (including staff and leadership) within the domain to understand and, where appropriate, to effectively use and manage AI and Large Language Models. It includes training programs, workshops, and other educational opportunities to build competency in AI-related technologies.\n\n\n2.2.5 Project Management & Support Personnel\nThis workstream is responsible for overseeing the implementation of AI and Large Language Models across the four domains. It includes project management and support staff who work collaboratively to ensure that AI integration occurs smoothly and efficiently.\nThe implementation plan is structured in a way that allows for cross-functional collaboration between the domains and workstreams. This ensures that AI and Large Language Models are integrated cohesively across the entire academic medical system, maximizing the benefits and minimizing potential risks."
  },
  {
    "objectID": "guiding_principles.html#principle-1-ai-tools-should-aim-to-alleviate-existing-health-disparities",
    "href": "guiding_principles.html#principle-1-ai-tools-should-aim-to-alleviate-existing-health-disparities",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.1 Principle 1: AI tools should aim to alleviate existing health disparities",
    "text": "3.1 Principle 1: AI tools should aim to alleviate existing health disparities\nReaching health equity requires eliminating the disparitities in health outcomes that are closely linked with social, economic, and environmental disadvantages. At their very core, AI tools require collection of specialized and high-quality data, advanced computing infrastructure for use, capacity to purchase or partner models from commercial entities, and unique technical expertise, all of which are less likely available to healthcare systems that serve the most disadvantaged populations.\nMore careful training and model development that accounts for the unique needs of disadvantaged populations is needed to ensure that AI tools do not exacerbate existing health disparities. Creating equitable AI tools may require prioritizing simpler models for deployment, and the trade-off between balancing accuracy and equity can potentially be resolved by designing AI tools that can be easily tailored to the local population. AI tools designed to serve disadvantaged groups must not unnecessarily divert resources from higher priority areas and more effective interventions (Principle 4)."
  },
  {
    "objectID": "guiding_principles.html#principle-2-ai-tools-should-produce-clinically-meaningful-outcomes",
    "href": "guiding_principles.html#principle-2-ai-tools-should-produce-clinically-meaningful-outcomes",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.2 Principle 2: AI tools should produce clinically meaningful outcomes",
    "text": "3.2 Principle 2: AI tools should produce clinically meaningful outcomes\nAI tools should be evaluated based on their ability to improve clinically meaningful outcomes. The clinical benefit of AI tools should be defined in the context of the existing standard of care, and the AI tool should be evaluated against this standard. If AI practitioners do not define clinical metrics for clinical benefit a priori, they risk producing tools that clinicians cannot evaluate or use. Clinician partners of AI researchers should evaluate accuracy, fairness, and risks of overdiagnosis and overtreatment (Principle 3). They should also evaluate the healthcare value (Principle 4) along with the explainability and auditability of AI tools and models (note principles outlined in Table 3.1."
  },
  {
    "objectID": "guiding_principles.html#principle-3-ai-tools-should-reduce-overdiagnosis-and-overtreatment",
    "href": "guiding_principles.html#principle-3-ai-tools-should-reduce-overdiagnosis-and-overtreatment",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.3 Principle 3: AI tools should reduce overdiagnosis and overtreatment",
    "text": "3.3 Principle 3: AI tools should reduce overdiagnosis and overtreatment\nParticularly in the United States, overdiagnosis and overtreatment are major drivers of healthcare costs and patient harm. Overdiagnosis occurs when a disease is diagnosed that would not have caused symptoms or death in a patient’s lifetime. Overtreatment occurs when a patient is treated for a disease that would not have caused symptoms or death in a patient’s lifetime. AI tools should be carefully constructed with the spectrum of disease and interventions to result in decreased overdiagnosis and overtreatment."
  },
  {
    "objectID": "guiding_principles.html#principle-4-ai-tools-should-have-high-healthcare-value-and-avoid-diverting",
    "href": "guiding_principles.html#principle-4-ai-tools-should-have-high-healthcare-value-and-avoid-diverting",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.4 Principle 4: AI tools should have high healthcare value and avoid diverting",
    "text": "3.4 Principle 4: AI tools should have high healthcare value and avoid diverting\nresources from higher-priority areas\nAI tools applied in healthcare should result in the same outcomes for reduced cost or better outcomes for costs comparable to current costs. Costs to gather inputs, build, maintain, update, interpret, and deploy in clinical practice must be estimated and included in weighing the decisions around AI tool application. Note that what might be cost-effective, leading to high healthcare value, in one setting might be extremely cost-ineffective in settings where resources are scarce."
  },
  {
    "objectID": "guiding_principles.html#principle-5-ai-tools-should-incorporate-social-structural-environmental",
    "href": "guiding_principles.html#principle-5-ai-tools-should-incorporate-social-structural-environmental",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.5 Principle 5: AI tools should incorporate social, structural, environmental,",
    "text": "3.5 Principle 5: AI tools should incorporate social, structural, environmental,\nemotional, and psychological drivers of health"
  },
  {
    "objectID": "guiding_principles.html#principle-6-ai-tools-should-be-easily-tailored-to-the-local-population",
    "href": "guiding_principles.html#principle-6-ai-tools-should-be-easily-tailored-to-the-local-population",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.6 Principle 6: AI tools should be easily tailored to the local population",
    "text": "3.6 Principle 6: AI tools should be easily tailored to the local population"
  },
  {
    "objectID": "guiding_principles.html#principle-7-ai-tools-should-promote-a-learning-healthcare-system",
    "href": "guiding_principles.html#principle-7-ai-tools-should-promote-a-learning-healthcare-system",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.7 Principle 7: AI tools should promote a learning healthcare system",
    "text": "3.7 Principle 7: AI tools should promote a learning healthcare system"
  },
  {
    "objectID": "guiding_principles.html#principle-8-ai-tools-should-facilitate-shared-decision-making",
    "href": "guiding_principles.html#principle-8-ai-tools-should-facilitate-shared-decision-making",
    "title": "3  Guiding principles for AI in healthcare",
    "section": "3.8 Principle 8: AI tools should facilitate shared decision-making",
    "text": "3.8 Principle 8: AI tools should facilitate shared decision-making\n\n\n\n\nBadal, Kimberly, Carmen M Lee, and Laura J Esserman. 2023. “Guiding Principles for the Responsible Development of Artificial Intelligence Tools for Healthcare.” Communication & Medicine 3 (1): 47. https://doi.org/10.1038/s43856-023-00279-9."
  },
  {
    "objectID": "bill_of_rights.html#commentary-and-references",
    "href": "bill_of_rights.html#commentary-and-references",
    "title": "4  Whitehouse AI Bill(s) of rights",
    "section": "4.1 Commentary and references",
    "text": "4.1 Commentary and references\n\nOpportunities and blind spots in the White House’s blueprint for an AI Bill of Rights\n6 Reactions to the White House’s AI Bill of Rights The nonbinding principles are being both celebrated and vilified\nApplying the Blueprint for an AI Bill of Rights\n\nHow Does the White House AI Bill of Rights Apply to Healthcare?\n\nExperts from Mayo Clinic Platform and DLA Piper weigh in on how the White House’s Blueprint for an AI Bill of Rights may impact healthcare and health AI regulation.\n\n\nThe US AI Bill Of Rights Should Kickstart The Debate On Bias In Artificial Intelligence"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "5  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "policy.html#vision-statement",
    "href": "policy.html#vision-statement",
    "title": "6  DRAFT policy",
    "section": "6.1 Vision Statement",
    "text": "6.1 Vision Statement\n\nLLMs must be used in a manner consistent with the mission, vision, and values of the academic hospital system.\nThe use of LLMs must align with relevant legal and regulatory requirements, including but not limited to data privacy, security, and intellectual property laws.\nThe deployment of LLMs should prioritize patient safety, privacy, and wellbeing.\nLLMs must be used in a transparent manner, with users understanding the capabilities and limitations of the technology.\nContinuous improvement and evaluation of LLM usage should be prioritized to ensure ongoing alignment with organizational goals."
  },
  {
    "objectID": "policy.html#stakeholder-considerations",
    "href": "policy.html#stakeholder-considerations",
    "title": "6  DRAFT policy",
    "section": "6.2 Stakeholder Considerations",
    "text": "6.2 Stakeholder Considerations\n\n6.2.1 Patients\n\nLLMs should be used to augment patient care and improve outcomes, without replacing the human touch and empathy of healthcare providers.\nPatients must be informed about the use of LLMs in their care, and they should have the option to opt out if desired.\nPatient data used in LLM applications must be anonymized, encrypted, and securely stored to protect patient privacy.\n\n\n\n6.2.2 Healthcare Providers\n\nLLMs should be deployed to enhance clinical decision-making and efficiency without undermining the autonomy and expertise of healthcare providers.\nAdequate training and support should be provided to healthcare providers to ensure proper use and understanding of LLMs.\nFeedback from healthcare providers must be regularly solicited to improve LLM performance and usability.\n\n\n\n6.2.3 Researchers\n\nThe use of LLMs in research must adhere to ethical standards, including obtaining informed consent and minimizing potential harm.\nCollaboration between researchers and LLM developers should be encouraged to drive innovation and address specific research needs.\nResearch involving LLMs should be transparent and reproducible, with results and methodologies made available to the wider scientific community.\n\n\n\n6.2.4 Administrators and Support Staff\n\nLLMs should be deployed in administrative and support functions to improve efficiency, reduce costs, and enhance the overall quality of service.\nStaff should receive appropriate training and support to understand and utilize LLMs effectively.\nEmployee feedback should be actively sought to identify areas of improvement and potential new applications for LLMs."
  },
  {
    "objectID": "policy.html#monitoring-and-compliance",
    "href": "policy.html#monitoring-and-compliance",
    "title": "6  DRAFT policy",
    "section": "6.3 Monitoring and Compliance",
    "text": "6.3 Monitoring and Compliance\n\nA designated LLM Steering Committee, comprising representatives from various stakeholder groups, will be responsible for monitoring and enforcing compliance with this policy.\nPeriodic audits and assessments will be conducted to ensure adherence to this policy and identify areas for improvement.\nPolicy violations may result in disciplinary action, up to and including termination of employment or access to LLMs"
  },
  {
    "objectID": "implementation.html",
    "href": "implementation.html",
    "title": "7  A Framework for Implementing AI and Large Language Models",
    "section": "",
    "text": "8 Implementation\nThis section will provide you with a step-by-step guide to implementing ChatGPT within your organization Lindegaard (2023). It will cover the following topics:"
  },
  {
    "objectID": "implementation.html#domains",
    "href": "implementation.html#domains",
    "title": "7  A Framework for Implementing AI and Large Language Models",
    "section": "7.1 Domains",
    "text": "7.1 Domains\nThe implementation plan for integrating AI and Large Language Models into an academic medical system consists of four main domains:\n\n\nEducation\n\nThis domain includes all activities related to teaching, learning, and evaluation within the institution. It also encompasses the development of new educational programs and the management of existing ones.\n\n\n\nResearch\n\nThis domain focuses on the practice of the basic, clinical, and translational research programs within the institution. In addition, it includes the management of research grants, the development of new research programs, and the dissemination of research findings.\n\n\n\nClinical\n\nThis domain encompasses all activities related to patient care, including the management and implementation of clinical services, decision support and clinical decision-making, automation, and point-of-care or electronic patient support.\n\n\n\nBusiness Operations\n\nThis domain focuses on the management of the institution’s business operations, including finance, human resources, information technology, and facilities management. It also includes the development of new business processes and the management of existing ones."
  },
  {
    "objectID": "implementation.html#workstreams",
    "href": "implementation.html#workstreams",
    "title": "7  A Framework for Implementing AI and Large Language Models",
    "section": "7.2 Workstreams",
    "text": "7.2 Workstreams\nWithin each domain, we have identified five workstreams that are critical for the successful implementation of AI and Large Language Models. These workstreams are:\n\n\nData Access & Use\n\nThis workstream focuses on managing and optimizing data access, use, and sharing within the academic medical system. It ensures that data is available, reliable, and secure for AI integration and that the necessary infrastructure is in place to support data-driven activities.\n\n\n\nIT, Security & Infrastructure\n\nThis workstream addresses the technical aspects of AI integration, including the development and maintenance of IT systems, ensuring data security, and providing the necessary hardware and software infrastructure to support AI and Large Language Models.\n\n\n\nEthical, Legal, & Social\n\nThis workstream focuses on the ethical, legal, and social implications of AI integration in the academic medical system with domain-specific focus as appropriate. It aims to ensure that AI is used responsibly and ethically and that any legal and social concerns are addressed proactively.\n\n\n\nTraining & Workforce Development\n\nThis workstream is dedicated to developing the skills and knowledge of domain community members (including staff and leadership) within the domain to understand and, where appropriate, to effectively use and manage AI and Large Language Models. It includes training programs, workshops, and other educational opportunities to build competency in AI-related technologies.\n\n\n\nProject Management & Support Personnel\n\nThis workstream is responsible for overseeing the implementation of AI and Large Language Models across the four domains. It includes project management and support staff who work collaboratively to ensure that AI integration occurs smoothly and efficiently.\n\n\n\nThe implementation plan is structured in a way that allows for cross-functional collaboration between the domains and workstreams. This ensures that AI and Large Language Models are integrated cohesively across the entire academic medical system, maximizing the benefits and minimizing potential risks."
  },
  {
    "objectID": "implementation.html#barriers-and-obstacles-and-how-to-overcome-them",
    "href": "implementation.html#barriers-and-obstacles-and-how-to-overcome-them",
    "title": "7  A Framework for Implementing AI and Large Language Models",
    "section": "8.1 Barriers and Obstacles and How to Overcome Them",
    "text": "8.1 Barriers and Obstacles and How to Overcome Them\nInevitably, you will encounter barriers and obstacles during your ChatGPT implementation journey. Recognizing these challenges and understanding how to overcome them is essential to ensure a successful adoption. \nIn this section, we will discuss the most common hurdles and provide strategies for navigating them, empowering you to lead your organization through the complexities of AI integration.\nResistance to Change\nEmployees and leaders may be resistant to adopting new technologies due to fear of job loss or discomfort with the unknown. To overcome this, emphasize the benefits of ChatGPT, such as increased efficiency and improved decision-making, and provide ample training and support. Encourage open discussions and showcase successful examples of AI adoption.\nLack of Technical Expertise\nLimited knowledge of AI and ChatGPT may hinder successful implementation. Address this by investing in training programs, partnering with AI experts, or hiring professionals with relevant experience. Create an internal AI community for knowledge sharing and peer support.\nInsufficient Collaboration\nInadequate communication and collaboration between departments can impede progress. Foster cross-functional teamwork through regular meetings, workshops, and collaborative platforms. Encourage leaders to champion the initiative and create a culture of cooperation.\nResource Constraints\nLimited budget, time, or personnel can pose challenges. To overcome this, prioritize use cases based on potential impact and feasibility, and secure buy-in from top management for necessary resources. Consider leveraging external partnerships or outsourcing certain tasks to reduce internal workload.\nData Privacy and Security Concerns\nHandling sensitive proprietary data may raise concerns. Collaborate closely with IT and Legal departments to establish robust data security protocols and comply with regulations. Communicate these measures transparently to build trust among employees and stakeholders.\nEthical Concerns\nThe potential for biased or unethical AI outcomes may create apprehension. Develop guidelines for responsible AI usage, create an ethics review board, and offer training on potential risks and challenges. Emphasize the importance of ethical AI practices throughout the organization.\nAs you reflect on the potential barriers and obstacles to ChatGPT implementation, remember that overcoming these challenges is an integral part of the journey towards AI-driven success. By anticipating and addressing these issues proactively, you can foster a resilient and adaptable organization that is well-prepared to navigate the ever-evolving landscape of artificial intelligence."
  },
  {
    "objectID": "implementation.html#overcoming-barriers-and-obstacles",
    "href": "implementation.html#overcoming-barriers-and-obstacles",
    "title": "7  A Framework for Implementing AI and Large Language Models",
    "section": "8.2 Overcoming Barriers and Obstacles",
    "text": "8.2 Overcoming Barriers and Obstacles\nAs you can see above, there are many barriers and obstacles to ChatGPT implementation and overcoming these is crucial for realizing its full potential within your organization. \nIn this section, we will provide a combination of conventional and less traditional strategies to address the challenges you might face during the adoption process. By embracing these adaptive approaches, you can foster a culture of adaptability and resilience, enabling your organization to successfully harness the power of AI-driven solutions like ChatGPT.\n\n8.2.1 Conventional Strategies for Overcoming Barriers and Obstacles\n\nResistance to Change: To overcome resistance to change, emphasize the benefits of ChatGPT, provide ample training and support, encourage open discussions, and showcase successful examples of AI adoption.\nLack of Technical Expertise: Address this by investing in training programs, partnering with AI experts, hiring professionals with relevant experience, and creating an internal AI community for knowledge sharing and peer support.\nInsufficient Collaboration: Foster cross-functional teamwork through regular meetings, workshops, and collaborative platforms, and encourage leaders to champion the initiative and create a culture of cooperation.\nResource Constraints: Prioritize use cases based on potential impact and feasibility, secure buy-in from top management for necessary resources, and consider leveraging external partnerships or outsourcing certain tasks to reduce internal workload.\nData Privacy and Security Concerns: Collaborate closely with IT and Legal departments to establish robust data security protocols and comply with regulations, and communicate these measures transparently to build trust among employees and stakeholders.\nEthical Concerns: Develop guidelines for responsible AI usage, create an ethics review board, and offer training on potential risks and challenges, emphasizing the importance of ethical AI practices throughout the organization.\n\n\n\n8.2.2 Unconventional Strategies for Overcoming Barriers and Obstacles\nI often find that we need to find “back-doors” and just different approaches in the context of change and transformation projects. Thus, here are some less conventional approaches to the barriers. \n\nGamification: Introduce gamification elements to the training and adoption process, incentivizing employees to engage with ChatGPT and learn its capabilities. Offer rewards or recognition for participation and achievements.\nReverse Mentoring: Encourage younger or more tech-savvy employees to mentor older or less experienced colleagues, facilitating knowledge sharing and promoting a more inclusive approach to technology adoption.\nInnovation Contests: Organize internal contests or hackathons for employees to develop creative ChatGPT use cases or solutions, fostering a sense of ownership and excitement around the technology.\nExternal Showcasing: Publicly share successful ChatGPT implementation stories or use cases to build a positive reputation, attract talent, and create a culture of innovation within the organization.\nAI Sabbaticals: Offer employees the opportunity to take short-term sabbaticals to focus on AI-related projects or training, providing dedicated time for learning and exploration. This can help develop in-house expertise and promote a culture of continuous learning.\n\n\n\n\n\nLindegaard, Stefan. 2023. “LinkedIn.” https://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/. https://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Badal, Kimberly, Carmen M Lee, and Laura J Esserman. 2023.\n“Guiding Principles for the Responsible Development of Artificial\nIntelligence Tools for Healthcare.” Communication &\nMedicine 3 (1): 47. https://doi.org/10.1038/s43856-023-00279-9.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLindegaard, Stefan. 2023. “LinkedIn.” https://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/.\nhttps://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/."
  },
  {
    "objectID": "appendices.html#ai-principles-proposed-by-select-organizations",
    "href": "appendices.html#ai-principles-proposed-by-select-organizations",
    "title": "Appendices",
    "section": "AI principles proposed by select organizations",
    "text": "AI principles proposed by select organizations\nThis list is adapted from Badal, Lee, and Esserman (2023), Table 1.\n\nEthics and governance of artificial intelligence for health, World Health Organization\n\nHuman autonomy\nHuman well-being and safety and the public interest\nTransparency, explainability, and intelligibility\nResponsibility and accountability\nInclusiveness and equity\nResponsive and sustainable\n\nMinistries of Health, Medical AI algorithm assessment checklist, FUTURE-AI (an international, multi-stakeholder consortium)\n\nFairness\nUniversality\nTraceability\nUsability\nRobustness\nExplainability\n\nGood Machine Learning Practice for Medical Device Development: Guiding Principles, fdFDA, Health Canada, United Kingdom’s Medicines and Healthcare products Regulatory Agency (MHRA)\n\nLeverage multidisciplinary expertise in development\nImplement good software engineering and security practices\nDatasets are representative of intended population\nTraining and test sets are independent\nReference datasets are well developed\nOptimize performance of Human-AI Team\nThorough clinical testing\nInformation accessible to users\nMonitor deployed models and mitigate retraining risk\n\nDefining AMIA’s artificial intelligence principles, American Medical Informatics Association (AMIA)\n\nAutonomy\nBeneficence\nNon-maleficence\nJustice\nExplainability\nInterpretability\nFairness\nDependability\nAuditability\nKnowledge managemen\n\n\n\n\n\n\nBadal, Kimberly, Carmen M Lee, and Laura J Esserman. 2023. “Guiding Principles for the Responsible Development of Artificial Intelligence Tools for Healthcare.” Communication & Medicine 3 (1): 47. https://doi.org/10.1038/s43856-023-00279-9."
  }
]