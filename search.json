[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Large Language Model Policy and Practice",
    "section": "",
    "text": "Overview\nLarge language models, such as ChatGPT, have gained significant attention in recent years due to their ability to generate human-like language and process vast amounts of natural language data. The development of these models presents a significant opportunity to transform healthcare by enhancing clinical decision-making, patient care, and medical research. However, the implementation of these models also presents several challenges, including technical, ethical, legal, and social challenges."
  },
  {
    "objectID": "index.html#technical-challenges-and-opportunities",
    "href": "index.html#technical-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "Technical Challenges and Opportunities",
    "text": "Technical Challenges and Opportunities\nOne of the significant challenges in developing and implementing large language models is the computational power required for training and inference. These models require a vast amount of data and computational resources to train and fine-tune. However, recent advancements in deep learning frameworks and cloud computing have made it possible to train and deploy large language models on a large scale.\nAnother technical challenge is the issue of bias in language models. Language models learn from the data they are trained on, which means that if the data is biased, the model will also be biased. This can have significant implications for healthcare, where biased models could lead to incorrect clinical decision-making or reinforce health disparities. To address this challenge, researchers have proposed several techniques to mitigate bias in language models, such as data augmentation, adversarial training, and fairness constraints (1)."
  },
  {
    "objectID": "index.html#ethical-challenges-and-opportunities",
    "href": "index.html#ethical-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "Ethical Challenges and Opportunities",
    "text": "Ethical Challenges and Opportunities\nThe implementation of large language models in healthcare raises several ethical concerns, such as patient privacy, informed consent, and fairness. Language models require vast amounts of data, including personal health information, which raises concerns about patient privacy and data protection. Additionally, patients may not understand how their data is being used or may not have given informed consent for their data to be used in this way.\nHowever, there are also significant ethical opportunities presented by the use of large language models in healthcare. For example, language models can be used to generate natural language explanations for clinical decision-making, which can improve transparency and help build trust between patients and healthcare providers. Additionally, language models can be used to identify and address health disparities by analyzing large-scale healthcare data and developing targeted interventions."
  },
  {
    "objectID": "index.html#legal-challenges-and-opportunities",
    "href": "index.html#legal-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "Legal Challenges and Opportunities",
    "text": "Legal Challenges and Opportunities\nThe implementation of large language models in healthcare also presents several legal challenges, such as liability and regulatory compliance. If language models are used to make clinical decisions, healthcare providers could be held liable for any adverse outcomes resulting from the use of these models. Additionally, healthcare providers must ensure that the use of language models complies with existing regulations, such as HIPAA, which govern the use of personal health information.\nHowever, there are also legal opportunities presented by the use of large language models in healthcare. For example, language models can be used to analyze large-scale healthcare data and identify potential areas of fraud or abuse, which can improve the efficiency and effectiveness of healthcare delivery."
  },
  {
    "objectID": "index.html#social-challenges-and-opportunities",
    "href": "index.html#social-challenges-and-opportunities",
    "title": "Large Language Model Policy and Practice",
    "section": "Social Challenges and Opportunities",
    "text": "Social Challenges and Opportunities\nThe implementation of large language models in healthcare also presents several social challenges, such as the potential for job displacement and the exacerbation of existing healthcare disparities. The use of language models could lead to the displacement of healthcare workers whose jobs can be automated by these models. Additionally, if language models are biased, they could reinforce existing healthcare disparities, particularly in marginalized communities.\nHowever, there are also social opportunities presented by the use of large language models in healthcare. For example, language models can be used to improve the accessibility or utilization of healthcare services in at-risk or disadvantaged populations. Additionally, language models can be used to improve the quality of healthcare services by providing personalized treatment recommendations and identifying potential areas of improvement in healthcare delivery or clinical operations.\nThe implementation of large language models in healthcare presents significant challenges and opportunities, including technical, ethical, legal, and social challenges. However, with careful consideration and mitigation of these challenges, the use of language models in healthcare has the potential to transform healthcare delivery and improve patient outcomes. It is essential for healthcare organizations to consider the potential risks and benefits of implementing large language models and to prioritize ethical and responsible use of these models.\nAs healthcare providers increasingly rely on large language models for clinical decision-making, it is critical to ensure that these models are transparent, explainable, and unbiased. Researchers and developers must work closely with healthcare providers and patients to ensure that the development and implementation of language models are aligned with ethical principles and patient needs.\nIn summary, the implementation of large language models in healthcare is a complex and rapidly evolving field with both challenges and opportunities. By carefully considering the technical, ethical, legal, and social implications of these models, healthcare organizations can leverage the full potential of large language models to improve patient outcomes and advance medical research."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "guiding_principles.html#principle-1-ai-tools-should-aim-to-alleviate-existing-health-disparities",
    "href": "guiding_principles.html#principle-1-ai-tools-should-aim-to-alleviate-existing-health-disparities",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.1 Principle 1: AI tools should aim to alleviate existing health disparities",
    "text": "2.1 Principle 1: AI tools should aim to alleviate existing health disparities\nReaching health equity requires eliminating the disparitities in health outcomes that are closely linked with social, economic, and environmental disadvantages. At their very core, AI tools require collection of specialized and high-quality data, advanced computing infrastructure for use, capacity to purchase or partner models from commercial entities, and unique technical expertise, all of which are less likely available to healthcare systems that serve the most disadvantaged populations.\nMore careful training and model development that accounts for the unique needs of disadvantaged populations is needed to ensure that AI tools do not exacerbate existing health disparities. Creating equitable AI tools may require prioritizing simpler models for deployment, and the trade-off between balancing accuracy and equity can potentially be resolved by designing AI tools that can be easily tailored to the local population. AI tools designed to serve disadvantaged groups must not unnecessarily divert resources from higher priority areas and more effective interventions (Principle 4)."
  },
  {
    "objectID": "guiding_principles.html#principle-2-ai-tools-should-produce-clinically-meaningful-outcomes",
    "href": "guiding_principles.html#principle-2-ai-tools-should-produce-clinically-meaningful-outcomes",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.2 Principle 2: AI tools should produce clinically meaningful outcomes",
    "text": "2.2 Principle 2: AI tools should produce clinically meaningful outcomes\nAI tools should be evaluated based on their ability to improve clinically meaningful outcomes. The clinical benefit of AI tools should be defined in the context of the existing standard of care, and the AI tool should be evaluated against this standard. If AI practitioners do not define clinical metrics for clinical benefit a priori, they risk producing tools that clinicians cannot evaluate or use. Clinician partners of AI researchers should evaluate accuracy, fairness, and risks of overdiagnosis and overtreatment (Principle 3). They should also evaluate the healthcare value (Principle 4) along with the explainability and auditability of AI tools and models (note principles outlined in Table 2.1."
  },
  {
    "objectID": "guiding_principles.html#principle-3-ai-tools-should-reduce-overdiagnosis-and-overtreatment",
    "href": "guiding_principles.html#principle-3-ai-tools-should-reduce-overdiagnosis-and-overtreatment",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.3 Principle 3: AI tools should reduce overdiagnosis and overtreatment",
    "text": "2.3 Principle 3: AI tools should reduce overdiagnosis and overtreatment\nParticularly in the United States, overdiagnosis and overtreatment are major drivers of healthcare costs and patient harm. Overdiagnosis occurs when a disease is diagnosed that would not have caused symptoms or death in a patient’s lifetime. Overtreatment occurs when a patient is treated for a disease that would not have caused symptoms or death in a patient’s lifetime. AI tools should be carefully constructed with the spectrum of disease and interventions to result in decreased overdiagnosis and overtreatment."
  },
  {
    "objectID": "guiding_principles.html#principle-4-ai-tools-should-have-high-healthcare-value-and-avoid-diverting",
    "href": "guiding_principles.html#principle-4-ai-tools-should-have-high-healthcare-value-and-avoid-diverting",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.4 Principle 4: AI tools should have high healthcare value and avoid diverting",
    "text": "2.4 Principle 4: AI tools should have high healthcare value and avoid diverting\nresources from higher-priority areas\nAI tools applied in healthcare should result in the same outcomes for reduced cost or better outcomes for costs comparable to current costs. Costs to gather inputs, build, maintain, update, interpret, and deploy in clinical practice must be estimated and included in weighing the decisions around AI tool application. Note that what might be cost-effective, leading to high healthcare value, in one setting might be extremely cost-ineffective in settings where resources are scarce."
  },
  {
    "objectID": "guiding_principles.html#principle-5-ai-tools-should-incorporate-social-structural-environmental",
    "href": "guiding_principles.html#principle-5-ai-tools-should-incorporate-social-structural-environmental",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.5 Principle 5: AI tools should incorporate social, structural, environmental,",
    "text": "2.5 Principle 5: AI tools should incorporate social, structural, environmental,\nemotional, and psychological drivers of health"
  },
  {
    "objectID": "guiding_principles.html#principle-6-ai-tools-should-be-easily-tailored-to-the-local-population",
    "href": "guiding_principles.html#principle-6-ai-tools-should-be-easily-tailored-to-the-local-population",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.6 Principle 6: AI tools should be easily tailored to the local population",
    "text": "2.6 Principle 6: AI tools should be easily tailored to the local population"
  },
  {
    "objectID": "guiding_principles.html#principle-7-ai-tools-should-promote-a-learning-healthcare-system",
    "href": "guiding_principles.html#principle-7-ai-tools-should-promote-a-learning-healthcare-system",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.7 Principle 7: AI tools should promote a learning healthcare system",
    "text": "2.7 Principle 7: AI tools should promote a learning healthcare system"
  },
  {
    "objectID": "guiding_principles.html#principle-8-ai-tools-should-facilitate-shared-decision-making",
    "href": "guiding_principles.html#principle-8-ai-tools-should-facilitate-shared-decision-making",
    "title": "2  Guiding principles for AI in healthcare",
    "section": "2.8 Principle 8: AI tools should facilitate shared decision-making",
    "text": "2.8 Principle 8: AI tools should facilitate shared decision-making\n\n\n\n\nBadal, Kimberly, Carmen M Lee, and Laura J Esserman. 2023. “Guiding Principles for the Responsible Development of Artificial Intelligence Tools for Healthcare.” Communication & Medicine 3 (1): 47. https://doi.org/10.1038/s43856-023-00279-9."
  },
  {
    "objectID": "bill_of_rights.html#commentary-and-references",
    "href": "bill_of_rights.html#commentary-and-references",
    "title": "3  Whitehouse AI Bill(s) of rights",
    "section": "3.1 Commentary and references",
    "text": "3.1 Commentary and references\n\nOpportunities and blind spots in the White House’s blueprint for an AI Bill of Rights\n6 Reactions to the White House’s AI Bill of Rights The nonbinding principles are being both celebrated and vilified\nApplying the Blueprint for an AI Bill of Rights\n\nHow Does the White House AI Bill of Rights Apply to Healthcare?\n\nExperts from Mayo Clinic Platform and DLA Piper weigh in on how the White House’s Blueprint for an AI Bill of Rights may impact healthcare and health AI regulation.\n\n\nThe US AI Bill Of Rights Should Kickstart The Debate On Bias In Artificial Intelligence"
  },
  {
    "objectID": "policy.html#vision-statement",
    "href": "policy.html#vision-statement",
    "title": "4  DRAFT policy",
    "section": "4.1 Vision Statement",
    "text": "4.1 Vision Statement\n\nLLMs must be used in a manner consistent with the mission, vision, and values of the academic hospital system.\nThe use of LLMs must align with relevant legal and regulatory requirements, including but not limited to data privacy, security, and intellectual property laws.\nThe deployment of LLMs should prioritize patient safety, privacy, and wellbeing.\nLLMs must be used in a transparent manner, with users understanding the capabilities and limitations of the technology.\nContinuous improvement and evaluation of LLM usage should be prioritized to ensure ongoing alignment with organizational goals."
  },
  {
    "objectID": "policy.html#stakeholder-considerations",
    "href": "policy.html#stakeholder-considerations",
    "title": "4  DRAFT policy",
    "section": "4.2 Stakeholder Considerations",
    "text": "4.2 Stakeholder Considerations\n\n4.2.1 Patients\n\nLLMs should be used to augment patient care and improve outcomes, without replacing the human touch and empathy of healthcare providers.\nPatients must be informed about the use of LLMs in their care, and they should have the option to opt out if desired.\nPatient data used in LLM applications must be anonymized, encrypted, and securely stored to protect patient privacy.\n\n\n\n4.2.2 Healthcare Providers\n\nLLMs should be deployed to enhance clinical decision-making and efficiency without undermining the autonomy and expertise of healthcare providers.\nAdequate training and support should be provided to healthcare providers to ensure proper use and understanding of LLMs.\nFeedback from healthcare providers must be regularly solicited to improve LLM performance and usability.\n\n\n\n4.2.3 Researchers\n\nThe use of LLMs in research must adhere to ethical standards, including obtaining informed consent and minimizing potential harm.\nCollaboration between researchers and LLM developers should be encouraged to drive innovation and address specific research needs.\nResearch involving LLMs should be transparent and reproducible, with results and methodologies made available to the wider scientific community.\n\n\n\n4.2.4 Administrators and Support Staff\n\nLLMs should be deployed in administrative and support functions to improve efficiency, reduce costs, and enhance the overall quality of service.\nStaff should receive appropriate training and support to understand and utilize LLMs effectively.\nEmployee feedback should be actively sought to identify areas of improvement and potential new applications for LLMs."
  },
  {
    "objectID": "policy.html#monitoring-and-compliance",
    "href": "policy.html#monitoring-and-compliance",
    "title": "4  DRAFT policy",
    "section": "4.3 Monitoring and Compliance",
    "text": "4.3 Monitoring and Compliance\n\nA designated LLM Steering Committee, comprising representatives from various stakeholder groups, will be responsible for monitoring and enforcing compliance with this policy.\nPeriodic audits and assessments will be conducted to ensure adherence to this policy and identify areas for improvement.\nPolicy violations may result in disciplinary action, up to and including termination of employment or access to LLMs"
  },
  {
    "objectID": "implementation.html#barriers-and-obstacles-and-how-to-overcome-them",
    "href": "implementation.html#barriers-and-obstacles-and-how-to-overcome-them",
    "title": "5  Implementation",
    "section": "5.1 Barriers and Obstacles and How to Overcome Them",
    "text": "5.1 Barriers and Obstacles and How to Overcome Them\nInevitably, you will encounter barriers and obstacles during your ChatGPT implementation journey. Recognizing these challenges and understanding how to overcome them is essential to ensure a successful adoption. \nIn this section, we will discuss the most common hurdles and provide strategies for navigating them, empowering you to lead your organization through the complexities of AI integration.\nResistance to Change\nEmployees and leaders may be resistant to adopting new technologies due to fear of job loss or discomfort with the unknown. To overcome this, emphasize the benefits of ChatGPT, such as increased efficiency and improved decision-making, and provide ample training and support. Encourage open discussions and showcase successful examples of AI adoption.\nLack of Technical Expertise\nLimited knowledge of AI and ChatGPT may hinder successful implementation. Address this by investing in training programs, partnering with AI experts, or hiring professionals with relevant experience. Create an internal AI community for knowledge sharing and peer support.\nInsufficient Collaboration\nInadequate communication and collaboration between departments can impede progress. Foster cross-functional teamwork through regular meetings, workshops, and collaborative platforms. Encourage leaders to champion the initiative and create a culture of cooperation.\nResource Constraints\nLimited budget, time, or personnel can pose challenges. To overcome this, prioritize use cases based on potential impact and feasibility, and secure buy-in from top management for necessary resources. Consider leveraging external partnerships or outsourcing certain tasks to reduce internal workload.\nData Privacy and Security Concerns\nHandling sensitive proprietary data may raise concerns. Collaborate closely with IT and Legal departments to establish robust data security protocols and comply with regulations. Communicate these measures transparently to build trust among employees and stakeholders.\nEthical Concerns\nThe potential for biased or unethical AI outcomes may create apprehension. Develop guidelines for responsible AI usage, create an ethics review board, and offer training on potential risks and challenges. Emphasize the importance of ethical AI practices throughout the organization.\nAs you reflect on the potential barriers and obstacles to ChatGPT implementation, remember that overcoming these challenges is an integral part of the journey towards AI-driven success. By anticipating and addressing these issues proactively, you can foster a resilient and adaptable organization that is well-prepared to navigate the ever-evolving landscape of artificial intelligence."
  },
  {
    "objectID": "implementation.html#overcoming-barriers-and-obstacles",
    "href": "implementation.html#overcoming-barriers-and-obstacles",
    "title": "5  Implementation",
    "section": "5.2 Overcoming Barriers and Obstacles",
    "text": "5.2 Overcoming Barriers and Obstacles\nAs you can see above, there are many barriers and obstacles to ChatGPT implementation and overcoming these is crucial for realizing its full potential within your organization. \nIn this section, we will provide a combination of conventional and less traditional strategies to address the challenges you might face during the adoption process. By embracing these adaptive approaches, you can foster a culture of adaptability and resilience, enabling your organization to successfully harness the power of AI-driven solutions like ChatGPT.\n\n5.2.1 Conventional Strategies for Overcoming Barriers and Obstacles\n\nResistance to Change: To overcome resistance to change, emphasize the benefits of ChatGPT, provide ample training and support, encourage open discussions, and showcase successful examples of AI adoption.\nLack of Technical Expertise: Address this by investing in training programs, partnering with AI experts, hiring professionals with relevant experience, and creating an internal AI community for knowledge sharing and peer support.\nInsufficient Collaboration: Foster cross-functional teamwork through regular meetings, workshops, and collaborative platforms, and encourage leaders to champion the initiative and create a culture of cooperation.\nResource Constraints: Prioritize use cases based on potential impact and feasibility, secure buy-in from top management for necessary resources, and consider leveraging external partnerships or outsourcing certain tasks to reduce internal workload.\nData Privacy and Security Concerns: Collaborate closely with IT and Legal departments to establish robust data security protocols and comply with regulations, and communicate these measures transparently to build trust among employees and stakeholders.\nEthical Concerns: Develop guidelines for responsible AI usage, create an ethics review board, and offer training on potential risks and challenges, emphasizing the importance of ethical AI practices throughout the organization.\n\n\n\n5.2.2 Unconventional Strategies for Overcoming Barriers and Obstacles\nI often find that we need to find “back-doors” and just different approaches in the context of change and transformation projects. Thus, here are some less conventional approaches to the barriers. \n\nGamification: Introduce gamification elements to the training and adoption process, incentivizing employees to engage with ChatGPT and learn its capabilities. Offer rewards or recognition for participation and achievements.\nReverse Mentoring: Encourage younger or more tech-savvy employees to mentor older or less experienced colleagues, facilitating knowledge sharing and promoting a more inclusive approach to technology adoption.\nInnovation Contests: Organize internal contests or hackathons for employees to develop creative ChatGPT use cases or solutions, fostering a sense of ownership and excitement around the technology.\nExternal Showcasing: Publicly share successful ChatGPT implementation stories or use cases to build a positive reputation, attract talent, and create a culture of innovation within the organization.\nAI Sabbaticals: Offer employees the opportunity to take short-term sabbaticals to focus on AI-related projects or training, providing dedicated time for learning and exploration. This can help develop in-house expertise and promote a culture of continuous learning.\n\n\n\n\n\nLindegaard, Stefan. 2023. “LinkedIn.” https://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/. https://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Badal, Kimberly, Carmen M Lee, and Laura J Esserman. 2023.\n“Guiding Principles for the Responsible Development of Artificial\nIntelligence Tools for Healthcare.” Communication &\nMedicine 3 (1): 47. https://doi.org/10.1038/s43856-023-00279-9.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLindegaard, Stefan. 2023. “LinkedIn.” https://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/.\nhttps://www.linkedin.com/pulse/chatgpt-implementation-scaling-organization-your-guide-lindegaard/."
  },
  {
    "objectID": "appendices.html#ai-principles-proposed-by-select-organizations",
    "href": "appendices.html#ai-principles-proposed-by-select-organizations",
    "title": "Appendices",
    "section": "AI principles proposed by select organizations",
    "text": "AI principles proposed by select organizations\nThis list is adapted from Badal, Lee, and Esserman (2023), Table 1.\n\nEthics and governance of artificial intelligence for health, World Health Organization\n\nHuman autonomy\nHuman well-being and safety and the public interest\nTransparency, explainability, and intelligibility\nResponsibility and accountability\nInclusiveness and equity\nResponsive and sustainable\n\nMinistries of Health, Medical AI algorithm assessment checklist, FUTURE-AI (an international, multi-stakeholder consortium)\n\nFairness\nUniversality\nTraceability\nUsability\nRobustness\nExplainability\n\nGood Machine Learning Practice for Medical Device Development: Guiding Principles, fdFDA, Health Canada, United Kingdom’s Medicines and Healthcare products Regulatory Agency (MHRA)\n\nLeverage multidisciplinary expertise in development\nImplement good software engineering and security practices\nDatasets are representative of intended population\nTraining and test sets are independent\nReference datasets are well developed\nOptimize performance of Human-AI Team\nThorough clinical testing\nInformation accessible to users\nMonitor deployed models and mitigate retraining risk\n\nDefining AMIA’s artificial intelligence principles, American Medical Informatics Association (AMIA)\n\nAutonomy\nBeneficence\nNon-maleficence\nJustice\nExplainability\nInterpretability\nFairness\nDependability\nAuditability\nKnowledge managemen\n\n\n\n\n\n\nBadal, Kimberly, Carmen M Lee, and Laura J Esserman. 2023. “Guiding Principles for the Responsible Development of Artificial Intelligence Tools for Healthcare.” Communication & Medicine 3 (1): 47. https://doi.org/10.1038/s43856-023-00279-9."
  }
]